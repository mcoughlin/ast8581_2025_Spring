{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjHDThjFYHXq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10 (Monday), AST 8581 / PHYS 8581 / CSCI 8581 / STAT 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>\n",
    "\n",
    "With contributions totally ripped off from Siddharth Mishra-Sharma (MIT) and Deep  Chatterjee (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oxM74nYHXr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are we headed?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression -> Dimensionality Reduction  -> Principle Component Analysis -> Clustering / Density Estimation / Anomaly Detection -> Supervised Learning -> <b> Deep Learning </b> -> Introduction to Databases - SQL -> Introduction to Databases - NoSQL -> Introduction to Multiprocessing -> Introduction to GPUs -> Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade emcee corner pytorch-lightning tqdm nflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import basinhopping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation-based (likelihood-free) inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Simulation-based inference (SBI) is a powerful class of methods for performing inference in settings where the likelihood is computationally intractable, but simulations can be realized via forward modeling. \n",
    "\n",
    "In this lecture we will\n",
    "- Introduce the notion of an implicit likelihood, and how to leverage it to perform inference;\n",
    "- Look at a \"traditional\" method for likelihood-free inference, Approximate Bayesian Computation (ABC);\n",
    "- Build up two common modern _neural_ SBI techniques: neural likelihood-ratio estimation (NRE) and neural posterior estimation (NPE);\n",
    "- Introduce the concept of statistical coverage testing and calibration.\n",
    "\n",
    "As examples, we will look at a simple Gaussian-signal-on-power-law-background (\"bump hunt\"), where the likelihood is tractable, and a more complicated example of inferring a distribution of point sources, where the likelihood is computationally intractable. We will emphasize what it means for a likelihood to be computationally intractable/challenging and where the advantages of SBI come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/header.png alt= “” width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Likelihood-Free Inference (Simulation-Based Inference)\n",
    "\n",
    "The problem of statistical inference in the absence of an explicit likelihood has been termed **likelihood-free inference** (though the term is somewhat of a misnomer). In reality, the goal is to estimate the intractable likelihood, so the term **simulation-based inference** is considered more appropriate.\n",
    "\n",
    "The intractability of the likelihood represents a barrier to scientific progress, as statistical inference is a crucial component of the scientific method. Over time, scientists have developed various methods to overcome this challenge, often using insights specific to their field.\n",
    "\n",
    "### Traditional Approaches to Inference\n",
    "\n",
    "1. **Density Estimation**: \n",
    "   This method involves approximating the distribution of summary statistics derived from the simulator. It has been widely used in fields such as high-energy physics (e.g., the discovery of the Higgs boson in a frequentist framework). The key idea is to estimate the distribution of the summary statistics based on samples generated from the simulator.\n",
    "\n",
    "2. **Approximate Bayesian Computation (ABC)**:\n",
    "   In ABC, observed and simulated data are compared using a distance measure based on summary statistics. ABC is widely applied in population biology, computational neuroscience, and cosmology. The method works by iteratively refining the simulations based on how close the simulated data is to the observed data.\n",
    "\n",
    "Both methods, despite their challenges, have been effective and widely used in many scientific domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recent Advances in Simulation-Based Inference\n",
    "\n",
    "The field of simulation-based inference (SBI) has seen rapid growth and expansion in recent years. Three key developments have been driving this acceleration:\n",
    "\n",
    "### 1. Cross-Pollination with Machine Learning\n",
    "A significant exchange of ideas has occurred between researchers in simulation-based inference and those in the machine learning (ML) community. The growth and advancements in machine learning techniques have provided new tools and methodologies that are enhancing simulation-based inference, allowing for novel approaches that were previously unfeasible.\n",
    "\n",
    "### 2. Active Learning\n",
    "Active learning, the process of using knowledge gained during inference to guide further simulations, has become increasingly recognized as a powerful tool. By focusing on regions of parameter space that are most informative, active learning improves the sample efficiency of inference methods, reducing computational costs and enhancing the accuracy of the results.\n",
    "\n",
    "### 3. Direct Integration with Simulators\n",
    "Recent research has shifted away from treating simulators as black boxes. Instead, there has been a focus on developing integrations that allow inference engines to directly access and utilize the internal details of simulators. This approach enables a more efficient and detailed use of the simulation data, further improving the quality of inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simulation-Based Inference and Statistical Models\n",
    "\n",
    "In simulation-based inference, the statistical model is defined by the simulator itself. A **simulator** is a computer program that takes a vector of parameters **θ**, samples internal states or latent variables **z_i** (which depend on previous latent variables), and produces a data vector **x** as output. The simulator represents a probabilistic program, where random samplings are involved and the program is interpreted as a statistical model.\n",
    "\n",
    "### Key Components of a Simulator\n",
    "\n",
    "A simulator in simulation-based inference can vary substantially depending on the context, but it typically includes the following components:\n",
    "\n",
    "#### 1. **Parameters (θ)**:\n",
    "   - These parameters describe the underlying mechanistic model that governs the system and affect the transition probabilities of the latent variables.\n",
    "   - **θ** typically has a small number of components and fixed dimensionality, and it is interpretable by domain scientists. Examples include:\n",
    "     - Coefficients in the Hamiltonian of a physical system.\n",
    "     - Virulence and incubation rates of a pathogen.\n",
    "     - Fundamental constants of nature.\n",
    "\n",
    "#### 2. **Latent Variables (z)**:\n",
    "   - These variables play a crucial role in the data-generating process but are typically unobservable in practice. The structure of the latent space can vary significantly between simulators:\n",
    "     - Latent variables may be **continuous** or **discrete**, and the dimensionality may be fixed or dynamic depending on the simulator's control flow.\n",
    "     - Simulators may combine **deterministic** and **stochastic** steps, with the deterministic components either being differentiable or involving discontinuous control flows.\n",
    "     - In practice, some simulators offer easy access to latent variables, while others are more like \"black boxes\" with limited transparency.\n",
    "   \n",
    "#### 3. **Output Data (x)**:\n",
    "   - The data produced by the simulator corresponds to the observations or outputs that are observed in the real-world scenario being modeled. These data can vary in complexity:\n",
    "     - From simple unstructured numbers to high-dimensional or highly structured data, such as images, time-series data, or geospatial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/sbi.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inference Tasks and Challenges in Simulation-Based Inference\n",
    "\n",
    "## Types of Inference Tasks\n",
    "\n",
    "Scientific inference tasks can vary depending on **what** is being inferred. Given observed data **x**, the goal may be to infer:\n",
    "- **Input parameters (θ)**: These are the parameters that define the system's behavior.\n",
    "- **Latent variables (z)**: These are variables that contribute to the data generation but are unobservable in practice.\n",
    "- **Both**: Sometimes, both parameters and latent variables are of interest.\n",
    "\n",
    "In some cases, only a subset of the parameters or latent variables may be of interest, while the rest are considered **nuisance parameters**. Nuisance parameters are not directly relevant to the inference but must be accounted for because they influence the distribution of the observed data.\n",
    "\n",
    "For this discussion, we will focus on the common problem of inferring **θ** in a parametric setting. We will also touch on methods for inferring **z**, but will not delve into non-parametric inverse problems.\n",
    "\n",
    "## Frequentist vs. Bayesian Inference\n",
    "\n",
    "Inference can be approached in two main ways:\n",
    "1. **Frequentist Inference**:\n",
    "   - Often focuses on point estimates **θ̂(x)**, with uncertainty represented as confidence sets.\n",
    "   - Confidence sets are typically derived by inverting hypothesis tests, using the likelihood ratio test statistic.\n",
    "\n",
    "2. **Bayesian Inference**:\n",
    "   - In Bayesian inference, the goal is to compute the posterior distribution:\n",
    "     $\n",
    "     p(\\theta | x) = \\frac{p(x | \\theta) \\, p(\\theta)}{\\int \\! \\! \\! \\text{d} \\theta' \\, p(x | \\theta') \\, p(\\theta')}\n",
    "     $\n",
    "     where **p(x | θ)** is the likelihood, **p(θ)** is the prior, and the denominator represents the marginal likelihood over all possible parameter values.\n",
    "   - Bayesian inference provides a probabilistic measure of uncertainty by calculating the full posterior distribution over the parameters **θ**.\n",
    "\n",
    "In both cases, the likelihood function **p(x | θ)** plays a central role.\n",
    "\n",
    "## The Challenge of Intractable Likelihoods\n",
    "\n",
    "In simulation-based inference, the **likelihood function** defined by the simulator is typically intractable. The likelihood is defined as an integral over all possible trajectories through the latent space, i.e., all possible execution traces of the simulator:\n",
    "\\[\n",
    "p(x | \\theta) = \\intz p(x, z | \\theta)\n",
    "\\]\n",
    "where **p(x, z | θ)** is the joint probability density of the observed data **x** and the latent variables **z**.\n",
    "\n",
    "For simple sequential data generation processes, the joint likelihood can be expressed as:\n",
    "\\[\n",
    "p(x, z | \\theta) = p(x | \\theta, z) \\prod_{i} p_i(z_i | \\theta, z_{<i})\n",
    "\\]\n",
    "However, for real-life simulators with large latent spaces, it becomes computationally impossible to calculate this integral explicitly. Since the likelihood function is central to both frequentist and Bayesian inference, this intractability presents a major challenge for inference in many scientific fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/likelihood.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classical Approaches to Inference Without Tractable Likelihoods\n",
    "\n",
    "The problem of performing inference without a tractable likelihood function is not new. Over the years, two major approaches have been developed to address this challenge.\n",
    "\n",
    "### 1. Approximate Bayesian Computation (ABC)\n",
    "\n",
    "One of the most well-known approaches is **Approximate Bayesian Computation (ABC)**, which was first introduced by Rubin (1984) and Beaumont (2002). In the simplest form of rejection ABC:\n",
    "- Parameters **θ** are drawn from the prior distribution.\n",
    "- The simulator is run with the chosen **θ** to sample **x_sim** from the likelihood distribution **p(x | θ)**.\n",
    "- The parameter **θ** is accepted as a posterior sample if the simulated data **x_sim** is sufficiently close to the observed data **x_obs**. This is determined by a distance measure **ρ(x_sim, x_obs)** and a tolerance **ε**.\n",
    "\n",
    "In essence, the likelihood is approximated by the probability that the condition **ρ(x_sim, x_obs) < ε** holds. The accepted samples then approximate the posterior distribution.\n",
    "\n",
    "### 2. Approximate Frequentist Computation\n",
    "\n",
    "The second classical approach to simulation-based inference involves creating a model for the likelihood by estimating the distribution of simulated data using techniques like histograms or kernel density estimation. Frequentist and Bayesian inference then proceed as if the likelihood were tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/abc.png alt= “” width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shortcomings of Traditional Simulation-Based Inference Techniques\n",
    "\n",
    "Traditional simulation-based inference techniques, such as **Approximate Bayesian Computation (ABC)** and **classical density estimation**, have been widely used in many fields. However, they suffer from three key shortcomings:\n",
    "\n",
    "### 1. Sample Efficiency\n",
    "- **Curse of Dimensionality**: Both ABC and classical density estimation suffer from poor scaling with high-dimensional data. This means that a large number of simulated samples is required to estimate the likelihood or posterior accurately.\n",
    "- The number of simulations needed to obtain reliable results can become prohibitively expensive, especially as the dimensionality of the data increases.\n",
    "\n",
    "### 2. Quality of Inference\n",
    "- **Information Loss**: Reducing the data to low-dimensional summary statistics inevitably discards some of the information about the parameters **θ**, leading to a loss in statistical power.\n",
    "- In **ABC**, large values of the **ε** parameter (the tolerance for accepting samples) lead to poor approximations of the true likelihood.\n",
    "- In **density estimation**, large bandwidths for kernel density estimation can also result in poor approximations.\n",
    "- Both issues reduce the overall **quality of inference**, making the results less reliable.\n",
    "\n",
    "### 3. Amortization\n",
    "- **Repetition of Steps**: With ABC, each new set of observed data requires repeating most steps of the inference process, especially if the proposal distribution depends on the observed data. This makes ABC inefficient when dealing with large datasets or multiple observations.\n",
    "- In contrast, **density estimation-based inference** is **amortized**, meaning the computationally expensive steps (like simulation and density estimation) are done once and can be reused for new data. This is particularly useful for problems with many i.i.d. observations, where repeated inference is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advances in Simulation-Based Inference\n",
    "\n",
    "Recent developments have significantly improved the three major challenges of traditional simulation-based inference: **sample efficiency, quality of inference, and amortization**. These advances can be grouped into three main directions of progress:\n",
    "\n",
    "## 1. Machine Learning Revolution\n",
    "- **Higher-Dimensional Data**: Modern deep learning techniques allow inference methods to work with high-dimensional data without relying on manually chosen summary statistics.\n",
    "- **Neural Network Surrogates**: Inference methods leveraging neural networks directly benefit from the rapid advancements in deep learning, improving the quality and accuracy of inference.\n",
    "\n",
    "## 2. Active Learning for Sample Efficiency\n",
    "- **Optimized Data Collection**: Active learning techniques iteratively guide the simulator to generate the most informative samples, improving sample efficiency.\n",
    "- **Reduction in Computational Costs**: By focusing simulation efforts on the most relevant regions of parameter space, active learning makes it feasible to tackle computationally expensive simulators.\n",
    "\n",
    "## 3. Integration of Differentiation and Probabilistic Programming\n",
    "- **Beyond Black-Box Simulators**: Advances in **automatic differentiation** and **probabilistic programming** allow inference methods to directly tap into the internal workings of the simulator.\n",
    "- **Augmenting Training Data**: Additional information extracted from the simulator (such as gradients) enhances inference, leading to more efficient and accurate probabilistic modeling.\n",
    "\n",
    "## Expanding the Frontier of Inference\n",
    "These advancements have significantly expanded the landscape of simulation-based inference, enabling researchers to handle **higher-dimensional data** and **more complex simulators** than ever before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/frontiers.png alt= “” width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Density Estimation in High Dimensions\n",
    "\n",
    "A major area of advancement in simulation-based inference is **density estimation in high-dimensional spaces**. Classical techniques such as **histograms** and **kernel density estimation** struggle with the curse of dimensionality, making them ineffective for complex datasets. Neural networks are now widely used to tackle this problem, leading to the development of **neural density estimation techniques**.\n",
    "\n",
    "## Normalizing Flows for Density Estimation\n",
    "One particularly powerful class of neural density estimation methods is **normalizing flows**. These models enable efficient and scalable density estimation through the following mechanism:\n",
    "\n",
    "1. **Base Distribution**: A simple, known probability distribution $p(u)$ (e.g., a multivariate Gaussian) is chosen as the starting point.\n",
    "2. **Invertible Transformations**: A parameterized, invertible function $x = g_\\phi(u)$ is applied to transform the base distribution into a more complex target distribution.\n",
    "3. **Change-of-Variables Formula**: The density of the transformed variable is computed as:\n",
    "   $\n",
    "   p_g(x) = p(u) \\left| \\det \\frac{d g_\\phi^{-1}(x)}{dx} \\right|\n",
    "   $\n",
    "   where the determinant of the Jacobian accounts for the transformation's effect on volume.\n",
    "4. **Stacking Transformations**: Multiple transformations can be composed in sequence, allowing the probability density to **“flow”** through different variable spaces, capturing intricate structures in the data.\n",
    "\n",
    "## Training Normalizing Flows\n",
    "- The parameters $\\phi$ of the transformations are optimized by **maximizing the likelihood** of the observed data under the model, ensuring that $p_g(x)$ closely approximates the unknown true density $p(x)$.\n",
    "- A key advantage of this approach is that it enables both **density estimation** (evaluating $p(x)$ and **data generation** (sampling new points by drawing $u$ from the base distribution and applying the learned transformations).\n",
    "\n",
    "## Conditional Neural Density Estimation\n",
    "- Normalizing flows and similar neural density estimators can be extended to **model conditional densities**, such as:\n",
    "  - **Likelihood estimation** $p(x|\\theta)$, where the model learns to approximate the probability of data given specific parameters.\n",
    "  - **Posterior estimation** $p(\\theta|x)$, where the model captures the distribution over parameters given observed data.\n",
    "- These extensions make neural density estimators particularly useful for simulation-based inference, enabling efficient and scalable probabilistic modeling.\n",
    "\n",
    "Neural density estimation is now a fundamental tool in high-dimensional statistical inference, opening the door to more expressive and scalable simulation-based inference techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/nre.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/npe.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Autoregressive Models for Density Estimation\n",
    "\n",
    "Another powerful class of neural density estimation techniques is **autoregressive models**. These models decompose a high-dimensional probability distribution into a sequence of **conditional densities** for individual components:\n",
    "\n",
    "$\n",
    "p(x) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2) \\dots p(x_d | x_{<d})\n",
    "$\n",
    "\n",
    "This factorization allows for **expressive** models with **tractable density evaluation** and the ability to generate synthetic data.\n",
    "\n",
    "## Key Features of Autoregressive Models\n",
    "- **Expressivity**: These models can capture complex dependencies between variables.\n",
    "- **Tractable Density**: The sequential decomposition ensures that each conditional probability can be explicitly computed.\n",
    "- **Synthetic Data Generation**: New samples can be drawn step-by-step, conditioning each step on the previously generated values.\n",
    "\n",
    "## Trade-offs and Applications\n",
    "- **Computational Cost**: Unlike some other neural density estimators, autoregressive models can be **slower** for sample generation since variables must be sampled sequentially.\n",
    "- **Alignment with Simulators**: Many scientific simulators naturally follow a sequential data generation process. This **structural similarity** between simulators and autoregressive models presents an opportunity to align **neural network latent variables** with the **semantically meaningful latent variables** of a simulator.\n",
    "\n",
    "Despite their computational drawbacks in industrial applications, autoregressive models offer a promising avenue for **simulation-based inference**, particularly when the latent variable structure of a simulator can be leveraged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generative Adversarial Networks (GANs) in Simulation-Based Inference\n",
    "\n",
    "**Generative Adversarial Networks (GANs)** offer an alternative approach to neural network-based generative modeling. Unlike normalizing flows or autoregressive models, GANs do not impose invertibility constraints on the transformation implemented by the generator. This increased **expressiveness** comes at the cost of an **intractable density function**, making maximum likelihood training infeasible.\n",
    "\n",
    "## How GANs Work\n",
    "GANs consist of two competing neural networks:\n",
    "- **Generator (G):** Learns to map random noise to realistic samples.\n",
    "- **Discriminator (D):** Attempts to distinguish between real samples and those generated by \\( G \\).\n",
    "\n",
    "The two networks are trained in a **minimax game**, where \\( G \\) tries to fool \\( D \\), and \\( D \\) tries to correctly classify real vs. generated data.\n",
    "\n",
    "## Implications for Simulation-Based Inference\n",
    "- **No Explicit Density Estimation**: Unlike normalizing flows or autoregressive models, GANs do not provide a tractable density function \\( p(x) \\).\n",
    "- **Flexible and High-Quality Generations**: The lack of invertibility constraints allows the generator to learn complex transformations, making GANs particularly powerful for **high-dimensional** or **structured** data.\n",
    "- **The Likelihood Ratio Trick**: A key insight in simulation-based inference is that the adversarial setup of GANs can be adapted to estimate likelihood ratios, enabling inference methods without directly computing the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workflows for Simulation-Based Inference\n",
    "\n",
    "## Overview\n",
    "Simulation-based inference methods vary in structure, with some closely resembling traditional approaches like Approximate Bayesian Computation (ABC) and density estimation, while others introduce fundamentally new workflows. To navigate this diverse landscape, we first outline common **building blocks** before assembling them into specific inference algorithms.\n",
    "\n",
    "## Common Components of Inference Workflows\n",
    "### 1. Running the Simulator\n",
    "- The simulator plays a central role in all inference methods (represented as a **yellow pentagon** in the next figure).\n",
    "- The parameters for simulation are drawn from a **proposal distribution**, which may:\n",
    "  - Be static or adaptively updated (e.g., using **active learning**).\n",
    "  - Depend on the prior in Bayesian settings.\n",
    "\n",
    "### 2. Data Processing\n",
    "- The high-dimensional simulator output can be:\n",
    "  - **Used directly** for inference.\n",
    "  - **Reduced to summary statistics**, which may be:\n",
    "    - **Prescribed** by experts.\n",
    "    - **Learned** from data.\n",
    "\n",
    "## Broad Categories of Inference Methods\n",
    "Inference methods can be divided into two main approaches:\n",
    "\n",
    "1. **Simulator-Dependent Methods (e.g., ABC)**  \n",
    "   - The simulator is directly used during inference.  \n",
    "   - The simulator output is compared to observed data (top panels of the next figure).\n",
    "\n",
    "2. **Surrogate-Based Methods**  \n",
    "   - The simulator generates training data for a separate estimation or machine learning model (green boxes in the figure).  \n",
    "   - A trained surrogate model (**red hexagons**) is used for inference.\n",
    "\n",
    "## Addressing Intractability of the Likelihood\n",
    "Different algorithms handle the challenge of an intractable likelihood function in various ways:\n",
    "- **Likelihood Estimation**: Construct a tractable surrogate for the likelihood.\n",
    "- **Likelihood Ratio Estimation**: Estimate the likelihood ratio function, facilitating frequentist inference.\n",
    "- **Implicit Inference**: Avoid explicit likelihood calculations, such as in rejection-based ABC methods (which do not naturally support frequentist inference).\n",
    "\n",
    "## Posterior Estimation in Bayesian Inference\n",
    "The ultimate goal in Bayesian inference is the posterior distribution. Different methods provide:\n",
    "- **Posterior samples** (e.g., from **MCMC or ABC**).\n",
    "- **Tractable approximations** of the posterior function.\n",
    "\n",
    "Additionally, workflows differ in **when** inference targets are specified:\n",
    "- Some require defining target quantities **early** in the process.\n",
    "- Others allow for flexibility, postponing this decision until later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/workflows10.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Amortized Inference and Surrogate Modeling\n",
    "\n",
    "## The Need for Amortization\n",
    "One key drawback of using the simulator directly during inference is the **lack of amortization**:\n",
    "- Each time new observed data becomes available, the **entire inference chain must be repeated**.\n",
    "- This makes direct simulation-based inference computationally inefficient, particularly for large datasets.\n",
    "\n",
    "By contrast, training a **surrogate model** or **emulator** for the simulator enables **amortized inference**:\n",
    "- After an **initially expensive** simulation and training phase, inference on new data becomes highly efficient.\n",
    "- This approach is particularly effective for **datasets with many i.i.d. observations**.\n",
    "\n",
    "## Inverting the Simulator: A Direct Approach\n",
    "A straightforward approach is to **invert the simulator**:\n",
    "- Train a model to predict the **true parameters** $\\hat{\\theta}(x)$ from observed data $(x\\).\n",
    "\n",
    "### Limitations of Direct Parameter Estimation\n",
    "Despite its simplicity, direct parameter estimation has drawbacks:\n",
    "- **Lack of probabilistic interpretation**: These methods provide **point estimates**, rather than **uncertainty estimates**.\n",
    "- **No likelihood or posterior modeling**: This limits their utility in Bayesian inference and confidence estimation.\n",
    "\n",
    "For these reasons, the focus is often on **learning probabilistic models** of the likelihood, likelihood ratio, or posterior, rather than direct inversion.\n",
    "\n",
    "---\n",
    "\n",
    "This discussion provides the foundation for various amortized inference workflows, illustrated in the **bottom panels of the above figure**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Surrogate Models for Simulation-Based Inference\n",
    "\n",
    "## Probabilistic Density Estimation\n",
    "A powerful approach to amortized inference is training **neural conditional density estimators** as **surrogates for the simulator**. These networks can approximate the conditional density in two ways:\n",
    "- **Posterior estimation**: Learn $p(\\theta | x)$ directly.\n",
    "- **Likelihood estimation**: Learn $p(x | \\theta)$ as a surrogate for the simulator.\n",
    "\n",
    "These techniques are illustrated in panel e and f of the above figure. Notably, the likelihood-based approach **resembles classical density estimation methods** but leverages more advanced machine learning techniques.\n",
    "\n",
    "## Learning the Likelihood Ratio\n",
    "An alternative to modeling densities directly is to train a neural network to estimate the **likelihood ratio**:\n",
    "- $\\frac{p(x|\\theta_0)}{p(x|\\theta_1)}$ (comparison of two parameter values).\n",
    "- $\\frac{p(x|\\theta_0)}{p(x)}$, where $p(x)$ is integrated over a proposal or prior.\n",
    "\n",
    "This approach (shown in panel g of the figure) is closely related to **GAN-based discriminators**:\n",
    "- A classifier is trained to **discriminate between data generated at two different parameter values $\\theta_0$ and $\\theta_1$**.\n",
    "- By leveraging the **Neyman-Pearson lemma**, the classifier output can be **converted into an estimate of the likelihood ratio**—a technique known as the **likelihood ratio trick**.\n",
    "\n",
    "## Advantages of Surrogate-Based Approaches\n",
    "These methods share key advantages:\n",
    "1. **Amortized inference**: After training, inference can be performed efficiently for **arbitrary data and parameters**.\n",
    "2. **No need for manual summary statistics**: The neural network learns the relevant structures in **high-dimensional data**.\n",
    "3. **Active learning potential**: By iteratively refining the proposal distribution, simulations can be **guided toward relevant parameter regions**, improving sample efficiency.\n",
    "\n",
    "## Choosing Between Likelihood, Likelihood Ratio, and Posterior Learning\n",
    "Despite their similarities, these approaches differ in key ways:\n",
    "\n",
    "| Method          | Bayesian or Frequentist? | Key Advantages | Key Drawbacks |\n",
    "|---------------|---------------------|-----------------|---------------|\n",
    "| **Posterior Estimation $p(\\theta | x)$** | Bayesian | Directly learns the target posterior | Prior-dependent at all inference stages |\n",
    "| **Likelihood Estimation $p(x | \\theta)$** | Both | Enables **frequentist inference** and **Bayesian inference** (with MCMC/VI) | Requires an additional step to generate posterior samples |\n",
    "| **Likelihood Ratio Estimation $\\frac{p(x | \\theta_0)}{p(x | \\theta_1)}$** | Both | More **flexible** (prior-independent), can be trained via **supervised learning** | Does not produce generative samples directly |\n",
    "\n",
    "## Supervised vs. Unsupervised Learning\n",
    "- **Likelihood and posterior estimation** are **unsupervised learning problems**.\n",
    "- **Likelihood ratio estimation** is a **supervised learning problem** (training a classifier), often making it a simpler and more efficient task.\n",
    "\n",
    "Since **likelihood and likelihood ratio methods are interchangeable for inference**, **learning the likelihood ratio function is often computationally advantageous**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Affine Autoregressive Transforms\n",
    "\n",
    "An **Affine Autoregressive Transform** is a type of **autoregressive flow** used in normalizing flows for **density estimation and generative modeling**. It defines a **learnable, invertible transformation** of a random variable where each dimension is transformed **sequentially** using an affine function whose parameters depend on the previously transformed dimensions.\n",
    "\n",
    "### Breaking It Down\n",
    "\n",
    "1. **Affine Transformation**  \n",
    "   Each component of the transformed variable is computed as:  \n",
    "   $\n",
    "   x_i = s_i(z_{1:i-1}) \\cdot z_i + t_i(z_{1:i-1})\n",
    "   $\n",
    "   where:\n",
    "   - $ x_i $ is the transformed variable,\n",
    "   - $ z_i $ is the input variable,\n",
    "   - $ s_i $ (scaling factor) and $ t_i $ (translation factor) are functions of the **previous** components $ z_{1:i-1} $,\n",
    "   - The transformation is **invertible** as long as $ s_i \\neq 0 $.\n",
    "\n",
    "2. **Autoregressive Structure**  \n",
    "   - The transformation is **sequential**: each output dimension $ x_i $ depends only on **previous** inputs $ z_{1:i-1} $.\n",
    "   - This factorization makes it possible to model complex **high-dimensional distributions**.\n",
    "\n",
    "3. **Use in Normalizing Flows**  \n",
    "   - Autoregressive flows allow tractable **density evaluation** via the change of variables formula:\n",
    "     $\n",
    "     p(x) = p(z) \\left| \\det J \\right|^{-1}\n",
    "     $\n",
    "     where $ J $ is the Jacobian of the transformation.\n",
    "   - Since the Jacobian is **triangular**, its determinant is just the **product of scaling factors** $ s_i $, making density computation **efficient**.\n",
    "\n",
    "### Examples of Affine Autoregressive Transforms\n",
    "- **MAF (Masked Autoregressive Flow)**: Uses deep neural networks to parameterize $ s_i $ and $ t_i $, ensuring the autoregressive property.\n",
    "- **IAF (Inverse Autoregressive Flow)**: Uses a similar structure but in **inverse mode**, making it fast for **sampling** rather than density evaluation.\n",
    "- **RealNVP (Real-Valued Non-Volume Preserving Flow)**: A simplified version using coupling layers, where only half of the dimensions depend on the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [The frontier of simulation-based inference](https://arxiv.org/abs/1911.01429) (Cranmer, Brehmer, Louppe): Review paper\n",
    "- [simulation-based-inference.org](http://simulation-based-inference.org/): List of papers and resources\n",
    "- [awesome-neural-sbi](https://github.com/smsharma/awesome-neural-sbi): List of papers and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class warm-up: Learning a multimodal distribution\n",
    "\n",
    "Affine Autoregressive Transforms to learn the transform from a standard normal into a two-moon distribution. The code is light and can be run on a local laptop; no GPUs needed.\n",
    "\n",
    "- Transforms are based on `pyro`, which closely embraces the `torch.distributions` library.\n",
    "- `sklearn` two-moon dataset is used.\n",
    "- `matplotlib` for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyro sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "from torch import distributions as dist\n",
    "from torch import optim\n",
    "\n",
    "from pyro.nn import AutoRegressiveNN\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.distributions.transforms import AffineAutoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples, labels = datasets.make_moons(n_samples=1000, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(samples.T[0], samples.T[1])\n",
    "plt.title(\"Two moon distribution\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples = torch.from_numpy(samples).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoregressive Net and Transform\n",
    "\n",
    "The flow we implement below has affine autoregressive transforms. Most of the constructs are available in the `pyro` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2  # data dimension\n",
    "hidden_dims = [50*input_dim, 50*input_dim, 50*input_dim]\n",
    "\n",
    "base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n",
    "\n",
    "arn = AutoRegressiveNN(input_dim, hidden_dims, param_dims=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "transform =  AffineAutoregressive(arn)  # the \"affine\" part implies the linear relation between hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the flow implementation is torch transformed distribution\n",
    "flow_dist = dist.TransformedDistribution(base_dist, [transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transform.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot(x_vals, y_vals, iteration):\n",
    "    \"\"\"Auxiliary function to visualize the distribution\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(x_vals, y_vals, label='proxy')\n",
    "    ax.scatter(samples.T[0], samples.T[1], alpha=0.1, label='Orig.')\n",
    "    ax.legend()\n",
    "    ax.set_title('iteration {}'.format(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learn the Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # take the original samples, and evaluate the likelihood.\n",
    "    loss = -flow_dist.log_prob(samples).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    flow_dist.clear_cache()  # pyro modules cache values and derivatives for performance\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            samples_flow = flow_dist.sample(torch.Size([1000,])).numpy()\n",
    "        live_plot(samples_flow[:,0], samples_flow[:,1], i + 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compose several transforms\n",
    "\n",
    "In the previous case we just had a single transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    AffineAutoregressive(\n",
    "        AutoRegressiveNN(input_dim, hidden_dims,\n",
    "                         param_dims=[1, 1])\n",
    "    ) for _ in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flow_dist = dist.TransformedDistribution(base_dist, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainable_parameters = []\n",
    "\n",
    "for t in transforms:\n",
    "    trainable_parameters.extend(list(t.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(trainable_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learn the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow_dist.log_prob(samples).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    flow_dist.clear_cache()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            samples_flow = flow_dist.sample(torch.Size([1000,])).numpy()\n",
    "\n",
    "        live_plot(samples_flow[:,0], samples_flow[:,1], i + 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_parameters = lambda parameters: sum(p.numel() for p in parameters if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Trainable parameters of single transform =\", num_parameters(transform.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Trainable parameters of after composing transforms =\", num_parameters(trainable_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Things to try\n",
    "\n",
    "- Compare results/number of trainable parameters from other flavors of autoregressive nets: splines, neural autoregressive etc.\n",
    "- Compare results/number of parameters with coupling layers instead. Note that like affine/spline autoregressive, there is the corresponding affine/spline coupling transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other exercises\n",
    "\n",
    "Depending on whether the masked feed-forward layers are implemented from \"data\" to \"normal\" direction or opposite, the flow is called masked-autoregressive or inverse-autoregressive. Look at the `pyro` source code on github and infer which one is the above implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## When the distribution is easy to evaluate\n",
    "\n",
    "In the exercise above we had \"samples\" from the the two-moon distribution, but did not have access to the true distribution. Consider the other regime, where we can evaluate the true distribution i.e. don't have samples. Train a flow to learn the following distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def multimodal_pdf(x, y):\n",
    "    res = np.exp(\n",
    "        - x**2 - (8 + 4*x**2 + 8*y)**2\n",
    "    )\n",
    "    res += 0.5*np.exp(\n",
    "        - 8*x**2 - 8*(y - 2)**2\n",
    "    )\n",
    "    res *= 16./3./np.pi\n",
    "    return np.log(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_vals = np.linspace(-3, 3, 100)\n",
    "y_vals = np.linspace(-3, 3, 100)\n",
    "\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "Z = multimodal_pdf(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "cs = ax.contourf(X, Y, np.exp(Z), levels=5)\n",
    "cbar = fig.colorbar(cs)\n",
    "\n",
    "ax.set_title(\"$p^{*}(x, y)$\")\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple bump-on-power-law example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an initial example, consider a Gaussian signal parameterized by {amplitude, mean location, std} on top of a power law background parameterize by {amplitude, power-law exponent}.\n",
    "$$ x_b = A_b\\,y^{n_b}$$\n",
    "$$x_s = A_s\\,\\exp^{-(y - \\mu_s)^2 / 2\\sigma_s^2}$$\n",
    "$$x \\sim \\mathrm{Pois}(x_b + x_s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n",
    "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n",
    "    \"\"\"\n",
    "    x_b = amp_b * (y ** exp_b)  # Power-law background\n",
    "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n",
    "\n",
    "    x = x_b + x_s  # Total mean signal\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def poisson_interval(k, alpha=0.32): \n",
    "    \"\"\" Uses chi2 to get the poisson interval.\n",
    "    \"\"\"\n",
    "    a = alpha\n",
    "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n",
    "    if k == 0: \n",
    "        low = 0.0\n",
    "    return k - low, high - k\n",
    "\n",
    "y = np.linspace(0.1, 1, 50)  # Dependent variable\n",
    "\n",
    "# Mean expected counts\n",
    "x_mu = bump_forward_model(y, \n",
    "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n",
    "                    amp_b=50, exp_b=-0.5)  # Background params\n",
    "\n",
    "# Realized counts\n",
    "x = np.random.poisson(x_mu)\n",
    "x_err = np.array([poisson_interval(k) for k in x.T]).T\n",
    "\n",
    "# Plot\n",
    "plt.plot(y, x_mu, color='k', ls='--', label=\"Mean expected counts\")\n",
    "plt.errorbar(y, x, yerr=x_err, fmt='o', color='k', label=\"Realized counts\")\n",
    "\n",
    "plt.xlabel(\"$y$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"assets/x1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The explicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, we can write down a log-likelihood as a Poisson over the mean returned by the forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like(theta, y, x):\n",
    "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n",
    "    \"\"\"\n",
    "    amp_s, mu_s, std_s, amp_b, exp_b = theta\n",
    "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    return poisson.logpmf(x, mu).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's focus on just 2 parameters for simplicity, the signal amplitude and mean location. The likelihood in this case is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like_sig(params, y, x):\n",
    "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = params\n",
    "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n",
    "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    return poisson.logpmf(x, mu).sum()\n",
    "\n",
    "log_like_sig([50, 0.8], y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a maximum-liklelihood estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Initial guess for the parameters\n",
    "initial_guess = [100., 0.1]\n",
    "\n",
    "# Set up the minimizer_kwargs for the basinhopping algorithm\n",
    "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1))}\n",
    "\n",
    "# Perform the optimization using basinhopping\n",
    "opt = basinhopping(lambda thetas: -log_like_sig(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n",
    "\n",
    "print(\"MLE parameters: {}; true parameters: {}\".format(opt.x, (50, 0.8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And approximate posterior using `emcee`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(thetas):\n",
    "    \"\"\" Log-prior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = thetas\n",
    "    if 0 < amp_s < 200 and 0 < mu_s < 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "def log_post(thetas, y, x):\n",
    "    \"\"\" Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    lp = log_prior(thetas)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return lp + log_like_sig(thetas, y, x)\n",
    "    \n",
    "# Sampling with `emcee`\n",
    "ndim, nwalkers = 2, 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(y, x))\n",
    "\n",
    "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler.run_mcmc(pos, 5000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot posterior samples\n",
    "flat_samples = sampler.get_chain(discard=1000, flat=True)\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], smooth=1.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The implicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we will do inference without relying on the explicit likelihood evaluation. The key realization is that samples from the forward model implicitly encode the likelihood; when we are simulating data points $x$ for different parameter points $\\theta$, we are drawing samples from the likelihood:\n",
    "$$x\\sim p(x\\mid\\theta)$$\n",
    "which is where the _implicit_ aspect comes from. Let's write down a bump simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def bump_simulator(thetas, y):\n",
    "    \"\"\" Simulate samples from the bump forward model given theta = (amp_s, mu_s) and abscissa points y.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = thetas\n",
    "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n",
    "    x_mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    x = np.random.poisson(x_mu)\n",
    "    return x\n",
    "\n",
    "# Test it out\n",
    "bump_simulator([50, 0.8], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approximate Bayesian Computation (ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea behind ABC is to realize samples from the forward model (with the parameters $\\theta$ drawn from a prior) and compare it to the dataset of interest $x$. If the data and realized samples are close enough to each other according to some criterion, we keep the parameter points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The comparison criterion here is a simple MSE on the data points. Play around with the parameters of the forward model to see how the criterion `eps` changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_fwd = bump_simulator([50, 0.8], y)\n",
    "eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def abc(y, x, eps_thresh=500., n_samples=1000):\n",
    "    \"\"\"ABC algorithm for Gaussian bump model.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Abscissa points.\n",
    "        x (np.ndarray): Data counts.\n",
    "        eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n",
    "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Accepted samples approximating the posterior p(theta|x).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    total_attempts = 0\n",
    "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n",
    "\n",
    "    # Keep simulating until we have enough accepted samples\n",
    "    while len(samples) < n_samples:\n",
    "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n",
    "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n",
    "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n",
    "        total_attempts += 1\n",
    "\n",
    "        # If accepted, add to samples\n",
    "        if eps < eps_thresh:\n",
    "            samples.append(params)\n",
    "            progress_bar.update(1)\n",
    "            acceptance_ratio = len(samples) / total_attempts\n",
    "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    return np.array(samples)\n",
    "\n",
    "n_samples = 5_000\n",
    "post_samples = abc(y, x, eps_thresh=200, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(post_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Downsides of vanilla ABC:\n",
    "- How to summarize the data? Curse of dimensionality / loss of information.\n",
    "- How to compare with data? Likelihood may not be available.\n",
    "- Choice of acceptance threshold: Precision/efficiency tradeoff.\n",
    "- Need to re-run pipeline for new data or new prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural likelihood-ratio estimation (NRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For numerical stability, the alternate hypothesis $\\theta_0$ can be assumed to be one where $x$ and $\\theta$ are not correlated, i.e., drawn from the individual marginal distributions $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. Then the alternate has support over the entire parameter space, instead of being a single hypothesis $\\theta_0$.\n",
    "\n",
    "In this case, we get the likelihood-to-evidence ratio,\n",
    "\n",
    "$$\\hat r(x, \\theta) = \\frac{s(x, \\theta)}{1 - s(x, \\theta)} = \\frac{p(x,\\theta)}{p(x)p(\\theta)} = \\frac{p(x\\mid\\theta)}{p(x)}$$\n",
    "\n",
    "$\\hat r(x, \\theta)$ can be shown to be the classifier logit, i.e. the output before softmaxxing into the decision function with range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Start by creating some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_train = 50_000\n",
    "\n",
    "# Simulate training data\n",
    "theta_samples = np.random.uniform(low=[0, 0], high=[200, 1], size=(n_train, 2))  # Parameter proposal\n",
    "x_samples = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples)])\n",
    "\n",
    "# Convert to torch tensors\n",
    "theta_samples = torch.tensor(theta_samples, dtype=torch.float32)\n",
    "x_samples = torch.tensor(x_samples, dtype=torch.float32)\n",
    "\n",
    "# Normalize the data\n",
    "x_mean = x_samples.mean(dim=0)\n",
    "x_std = x_samples.std(dim=0)\n",
    "x_samples = (x_samples - x_mean) / x_std\n",
    "\n",
    "theta_mean = theta_samples.mean(dim=0)\n",
    "theta_std = theta_samples.std(dim=0)\n",
    "theta_samples = (theta_samples - theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As our parameterized classifier, we will use a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, hidden_dim, output_dim, layers, activation=nn.GELU()):\n",
    "    \"\"\"Create an MLP from the configuration.\"\"\"\n",
    "    seq = [nn.Linear(input_dim, hidden_dim), activation]\n",
    "    for _ in range(layers):\n",
    "        seq += [nn.Linear(hidden_dim, hidden_dim), activation]\n",
    "    seq += [nn.Linear(hidden_dim, output_dim)]\n",
    "    return nn.Sequential(*seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create a neural ratio estimator class, with a corresponding loss function. The loss is a simple binary cross-entropy loss that discriminates between samples from the joint distribution $\\{x, \\theta\\} \\sim p(x\\mid\\theta)$ and those from a product of marginals $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. Samples from the latter are obtained by shuffling joint samples from within a batch.\n",
    "\n",
    "The binary cross-entropy loss is used as the classifier loss to distinguish samples from the joint and marginals,\n",
    "$$\\mathcal L = - \\sum_i y_i \\log(p_i)$$\n",
    "where $y_i$ are the true labels and $p_i$ the softmaxxed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralRatioEstimator(pl.LightningModule):\n",
    "    \"\"\" Simple neural likelihood-to-evidence ratio estimator, using an MLP as a parameterized classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, theta_dim):\n",
    "        super().__init__()\n",
    "        self.classifier = build_mlp(input_dim=x_dim + theta_dim, hidden_dim=128, output_dim=1, layers=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def loss(self, x, theta):\n",
    "\n",
    "        # Repeat x in groups of 2 along batch axis\n",
    "        x = x.repeat_interleave(2, dim=0)\n",
    "\n",
    "        # Get a shuffled version of theta\n",
    "        theta_shuffled = theta[torch.randperm(theta.shape[0])]\n",
    "\n",
    "        # Interleave theta and shuffled theta\n",
    "        theta = torch.stack([theta, theta_shuffled], dim=1).reshape(-1, theta.shape[1])\n",
    "\n",
    "        # Get labels; ones for pairs from joint, zeros for pairs from marginals\n",
    "        labels = torch.ones(x.shape[0], device=x.device) \n",
    "        labels[1::2] = 0.0\n",
    "\n",
    "        # Pass through parameterized classifier to get logits\n",
    "        logits = self(torch.cat([x, theta], dim=1))\n",
    "        probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "        return nn.BCELoss(reduction='none')(probs, labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate loss; initially it should be around -log(0.5) = 0.693\n",
    "nre = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n",
    "nre.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instantiate dataloader and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val_fraction = 0.1\n",
    "batch_size = 128\n",
    "n_samples_val = int(val_fraction * len(x_samples))\n",
    "\n",
    "dataset = TensorDataset(x_samples, theta_samples)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model=nre, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The classifier logits are now an estimator for the likelihood ratio. We can write down a log-likelihood function and use it to sample from the corresponding posterior distribution, just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like(theta, x):\n",
    "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n",
    "    \"\"\"\n",
    "        \n",
    "    x = torch.Tensor(x)\n",
    "    theta = torch.Tensor(theta)\n",
    "\n",
    "    # Normalize\n",
    "    x = (x - x_mean) / x_std\n",
    "    theta = (theta - theta_mean) / theta_std\n",
    "\n",
    "    x = torch.atleast_1d(x)\n",
    "    theta = torch.atleast_1d(theta)\n",
    "\n",
    "    return nre.classifier(torch.cat([x, theta], dim=-1)).squeeze()\n",
    "\n",
    "theta_test = np.array([90, 0.8])\n",
    "x_test = bump_simulator(theta_test, y)\n",
    "\n",
    "log_like(theta_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_post(theta, x):\n",
    "    \"\"\" Log-posterior distribution, for sampling.\n",
    "    \"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return lp + log_like(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sample with `emcee`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 2, 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n",
    "\n",
    "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler.run_mcmc(pos, 5000, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot approximate posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=1000, flat=True)\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural posterior estimation (NPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n",
    "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n",
    "    \"\"\"\n",
    "\n",
    "    base_dist = StandardNormal(shape=[d_in])\n",
    "\n",
    "    transforms = []\n",
    "    for _ in range(n_layers):\n",
    "        transforms.append(ReversePermutation(features=d_in))\n",
    "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n",
    "    transform = CompositeTransform(transforms)\n",
    "\n",
    "    flow = Flow(transform, base_dist)\n",
    "    return flow\n",
    "\n",
    "# Instantiate flow\n",
    "flow = get_flow()\n",
    "\n",
    "# Make sure sampling and log-prob calculation makes sense\n",
    "samples, log_prob = flow.sample_and_log_prob(num_samples=100, context=torch.randn(2, 16))\n",
    "print(samples.shape, log_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Construct a neural posterior estimator. It uses a normalizing flow as a (conditional) posterior density estimator, and a feature-extraction network that aligns the directions of variations in parameters $\\theta$ and data $x$.\n",
    "$$  \\mathcal L = -\\log p_\\phi(\\theta\\mid s_\\varphi(x))$$\n",
    "where $\\{\\phi, \\varphi\\}$ are the parameters of the normalizing flow and featurizer MLP, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralPosteriorEstimator(pl.LightningModule):\n",
    "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n",
    "    \"\"\"\n",
    "    def __init__(self, featurizer, d_context=16):\n",
    "        super().__init__()\n",
    "        self.featurizer = featurizer\n",
    "        self.flow = get_flow(d_in=2, d_hidden=32, d_context=d_context, n_layers=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.featurizer(x)\n",
    "    \n",
    "    def loss(self, x, theta):\n",
    "        context = self(x)\n",
    "        return -self.flow.log_prob(inputs=theta, context=context)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instantiate the NPE class and look at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = NeuralPosteriorEstimator(featurizer=build_mlp(input_dim=50, hidden_dim=128, output_dim=16, layers=4))\n",
    "npe.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Train using the same data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a test data sample, pass it through the feature extractor, and condition the flow density estimator on it. We get posterior samples by drawing from \n",
    "$$\\theta \\sim p_\\phi(\\theta\\mid s_\\varphi(x)).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta_test = np.array([90, 0.8])\n",
    "x_test = bump_simulator(theta_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "context = npe.featurizer(x_test_norm).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()\n",
    "corner.corner(samples_test, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more complicated example: distribution of point sources in a 2D image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let's look at a more complicated example, one that is closer to a typical application of SBI and where the likelihood is formally intractable.\n",
    "\n",
    "The forward model simulates a map of point sources with mean counts drawn from a power law (Pareto) distribution. The distribution of the mean counts is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm dn}{\\mathrm  ds} = A s^{\\beta}\n",
    "$$\n",
    "\n",
    "where $A$ is the amplitude (amp_b), $s$ is the flux, and $\\beta$ is the exponent (exp_b). The fluxes are drawn from a truncated power law with minimum and maximum bounds, $s_\\text{min}$ and $s_\\text{max}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of sources is determined by integrating the power law distribution within the flux limits and taking a Poisson realization:\n",
    "\n",
    "$$\n",
    "N_\\text{sources} \\sim \\text{Pois}\\left(\\int_{s_\\text{min}}^{s_\\text{max}} \\, \\mathrm ds \\frac{\\mathrm dn}{\\mathrm ds}\\right)\n",
    "$$\n",
    "\n",
    "For each source, a position is randomly assigned within the box of size `box_size`. The fluxes are then binned into a grid with `resolution` number of bins in both x and y directions. The resulting map is convolved with a Gaussian point spread function (PSF) with a standard deviation of `sigma_psf` to account for the spatial resolution of the instrument.\n",
    "\n",
    "The output is a 2D map of counts, representing the simulated observation of the point sources in the sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic_2d\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "\n",
    "\n",
    "def simulate_sources(amp_b, exp_b, s_min=0.5, s_max=50.0, box_size=1., resolution=64, sigma_psf=0.01):\n",
    "    \"\"\" Simulate a map of point sources with mean counts drawn from a power law (Pareto) distribution dn/ds = amp_b * s ** exp_b\n",
    "    \"\"\"\n",
    "    # Get number of sources by analytically integrating dn/ds and taking Poisson realization\n",
    "    n_sources = np.random.poisson(-amp_b * (s_min ** (exp_b - 1)) / (exp_b - 1))\n",
    "\n",
    "    # Draw fluxes from truncated power law amp_b * s ** (exp_b - 1), with s_min and s_max as the bounds\n",
    "    fluxes = draw_powerlaw_flux(n_sources, s_min, s_max, exp_b)\n",
    "\n",
    "    positions = np.random.uniform(0., box_size, size=(n_sources, 2))\n",
    "    bins = np.linspace(0, box_size, resolution + 1)\n",
    "\n",
    "    pixel_size = box_size / resolution\n",
    "    kernel = Gaussian2DKernel(x_stddev=1.0 * sigma_psf / pixel_size)\n",
    "\n",
    "    mu_signal = binned_statistic_2d(x=positions[:, 0], y=positions[:, 1], values=fluxes, statistic='sum', bins=bins).statistic\n",
    "    counts = np.random.poisson(convolve(mu_signal, kernel))\n",
    "                \n",
    "    return fluxes, counts\n",
    "\n",
    "def draw_powerlaw_flux(n_sources, s_min, s_max, exp_b):\n",
    "    \"\"\"\n",
    "    Draw from a powerlaw with slope `exp_b` and min/max mean counts `s_min` and `s_max`. From:\n",
    "    https://stackoverflow.com/questions/31114330/python-generating-random-numbers-from-a-power-law-distribution\n",
    "    \"\"\"\n",
    "    u = np.random.uniform(0, 1, size=n_sources)\n",
    "    s_low_u, s_high_u = s_min ** (exp_b + 1), s_max ** (exp_b + 1)\n",
    "    return (s_low_u + (s_high_u - s_low_u) * u) ** (1.0 / (exp_b + 1.0))\n",
    "\n",
    "fluxes, counts = simulate_sources(amp_b=200., exp_b=-1.2)\n",
    "plt.imshow(counts, cmap='viridis', vmax=20)\n",
    "plt.xlabel(\"Pixels\")\n",
    "plt.ylabel(\"Pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw parameters from the prior\n",
    "n_params = 16\n",
    "\n",
    "amp_b_prior = (100., 300.)\n",
    "exp_b_prior = (-2.0, -0.5)\n",
    "\n",
    "amp_bs = np.random.uniform(amp_b_prior[0], amp_b_prior[1], n_params)\n",
    "exp_bs = np.random.uniform(exp_b_prior[0], exp_b_prior[1], n_params)\n",
    "\n",
    "# Plot the data samples on a grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fluxes, counts = simulate_sources(amp_b=amp_bs[i], exp_b=exp_bs[i])\n",
    "    im = ax.imshow(counts, cmap='viridis', vmax=20)\n",
    "    ax.set_title(f'$A_b={amp_bs[i]:.2f}, n_b={exp_bs[i]:.2f}$')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Explicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The (marginal) likelihood, which we would need to plug into something like MCMC, is computationally intractable! This is because it involves an integral over a cumbersome latent space, which consists of all possible number $n$ of sources and their positions $\\{z\\}=\\{x, y\\}_{i=1}^{n}$. Let's write this out formally:\n",
    "$$p(x \\mid \\theta)=\\sum_{n} \\int \\mathrm{d}^{n} \\{z\\}\\, p\\left(n \\mid \\theta\\right) \\prod_i^{n} p\\left(z_{i} \\mid \\theta\\right) \\, p\\left(x \\mid \\theta,\\left\\{z_{i}\\right\\}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implicit inference: Neural posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use neural posterior estimation with a normalizing flow again. Get a training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_train = 30_000\n",
    "\n",
    "# Sample from prior, then simulate\n",
    "theta_samples = np.random.uniform(low=[10., -3.], high=[200., -0.99], size=(n_train, 2))\n",
    "x_samples = np.array([simulate_sources(theta[0], theta[1])[1] for theta in tqdm(theta_samples)])\n",
    "\n",
    "# Convert to torch tensors\n",
    "theta_samples = torch.Tensor(theta_samples)\n",
    "x_samples = torch.Tensor(x_samples)\n",
    "\n",
    "# Normalize the data\n",
    "x_mean = x_samples.mean(dim=0)\n",
    "x_std = x_samples.std(dim=0)\n",
    "x_samples = (x_samples - x_mean) / x_std\n",
    "\n",
    "theta_mean = theta_samples.mean(dim=0)\n",
    "theta_std = theta_samples.std(dim=0)\n",
    "theta_samples = (theta_samples - theta_mean) / theta_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "val_fraction = 0.1\n",
    "batch_size = 128\n",
    "n_samples_val = int(val_fraction * len(x_samples))\n",
    "\n",
    "dataset = TensorDataset(x_samples, theta_samples)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we're working with images, use a simple convolutional neural network (CNN) as the feature extractor. The normalizing flow will be conditioned on the output of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\" Simple CNN feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.unsqueeze(1)  # Add channel dim\n",
    "        \n",
    "        x = self.pool1(F.leaky_relu(self.conv1(x), negative_slope=0.02))\n",
    "        x = self.pool2(F.leaky_relu(self.conv2(x), negative_slope=0.02))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = NeuralPosteriorEstimator(featurizer=CNN(output_dim=32), d_context=32)\n",
    "npe.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=15)\n",
    "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = npe.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a test map, extract features, condition normalizing flow, extract samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "params_test = np.array([15., -1.4])\n",
    "x_test = simulate_sources(params_test[0], params_test[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "context = npe.featurizer(x_test_norm.unsqueeze(0))\n",
    "\n",
    "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()\n",
    "\n",
    "corner.corner(samples_test, labels=[\"amp\", \"exp\"], truths=params_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test of statistical coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/coverage.png alt= “” width=800>\n",
    "\n",
    "Figure from 2209.01845."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do some checks to make sure that our posterior has the correct statistical interpretation. In particular, let's test the posterior statistical coverage by evaluating how well the Highest Posterior Density (HPD) intervals capture the true parameter values.\n",
    "\n",
    "The **Highest Posterior Density (HPD)** interval is a region in the parameter space that contains the most probable values for a given credible mass (e.g., 95% or 99%). In other words, it is the shortest interval that contains the specified credible mass of the posterior distribution. This is one of summarizing a posterior distribution.\n",
    "\n",
    "**Nominal coverage** is the probability, or the proportion of the parameter space, that the HPD interval is intended to contain. For example, if the nominal coverage is 0.95, the HPD interval should theoretically contain the true parameter value 95% of the time.\n",
    "\n",
    "**Empirical coverage** is the proportion of true parameter values that actually fall within the HPD interval, based on a set of test cases or simulations.\n",
    "\n",
    "For a perfectly calibrated posterior estimator, empirical coverage = nominal coverage for all credibility levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_test = 200  # How many test samples to draw for coverage test\n",
    "\n",
    "# Get samples \n",
    "x_test = torch.Tensor([simulate_sources(params_test[0], params_test[1])[1] for _ in range(n_test)])\n",
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "\n",
    "# and featurize\n",
    "context = npe.featurizer(x_test_norm)\n",
    "\n",
    "# Get posterior for all samples together in a batch\n",
    "samples_test = npe.flow.sample(num_samples=1000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hpd(samples, credible_mass=0.95):\n",
    "    \"\"\"Compute highest posterior density (HPD) of array for given credible mass.\"\"\"\n",
    "    sorted_samples = np.sort(samples)\n",
    "    interval_idx_inc = int(np.floor(credible_mass * sorted_samples.shape[0]))\n",
    "    n_intervals = sorted_samples.shape[0] - interval_idx_inc\n",
    "    interval_width = np.zeros(n_intervals)\n",
    "    for i in range(n_intervals):\n",
    "        interval_width[i] = sorted_samples[i + interval_idx_inc] - sorted_samples[i]\n",
    "    hdi_min = sorted_samples[np.argmin(interval_width)]\n",
    "    hdi_max = sorted_samples[np.argmin(interval_width) + interval_idx_inc]\n",
    "    return hdi_min, hdi_max\n",
    "\n",
    "hpd(samples_test[0, :, 0], credible_mass=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p_nominals = np.linspace(0.01, 0.99, 50)\n",
    "contains_true = np.zeros((2, n_test, len(p_nominals)))\n",
    "\n",
    "for i_param in range(2):\n",
    "    for i, sample in enumerate(samples_test[:, :, i_param]):\n",
    "        for j, p_nominal in enumerate(p_nominals):\n",
    "            hdi_min, hdi_max = hpd(sample, credible_mass=p_nominal)\n",
    "            if hdi_min < params_test[i_param] < hdi_max:\n",
    "                contains_true[i_param, i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Make two plots, one for each parameter\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(p_nominals, contains_true[0].sum(0) / n_test)\n",
    "ax[0].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "ax[0].set_xlabel(\"Nominal coverage\")\n",
    "ax[0].set_ylabel(\"Expected coverage\")\n",
    "ax[0].set_title(\"Coverage for amplitude\")\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(p_nominals, contains_true[1].sum(0) / n_test)\n",
    "ax[1].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "ax[1].set_xlabel(\"Nominal coverage\")\n",
    "ax[1].set_ylabel(\"Expected coverage\")\n",
    "ax[1].set_title(\"Coverage for exponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise: Damped Harmonic Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ddot{x}(t) + 2\\beta\\omega_0\\dot{x}(t) + \\omega_0^2x(t) = 0\n",
    "\\end{align}\n",
    "\n",
    "Ansatz, $x = \\exp(\\gamma t)$; $\\gamma = -\\omega_0\\left[\\beta \\pm i\\sqrt{1 - \\beta^2}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damped_sho(t, omega_0, beta, shift=0):\n",
    "    # beta less than 1 for underdamped\n",
    "    envel = beta * omega_0\n",
    "    osc = torch.sqrt(1 - beta**2) * omega_0\n",
    "    tau = t - shift\n",
    "    data = torch.exp(-envel * tau) * torch.cos(osc * tau)\n",
    "    data[tau < 0] = 0  # assume oscillator starts at tau = 0\n",
    "    return data\n",
    "\n",
    "def damped_sho_bilby(t, omega_0, beta, shift=0):\n",
    "    # beta less than 1 for underdamped\n",
    "    envel = beta * omega_0\n",
    "    osc = np.sqrt(1 - beta**2) * omega_0\n",
    "    tau = t - shift\n",
    "    data = np.exp(-envel * tau) * np.cos(osc * tau)\n",
    "    data[tau < 0] = 0  # assume oscillator starts at tau = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_0 = torch.tensor(1.94)\n",
    "beta = torch.tensor(0.4)\n",
    "\n",
    "t_vals = torch.linspace(-1, 10, 200)\n",
    "for shift in [-1, 0, 1, 2]:\n",
    "    x_vals = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    plt.plot(t_vals, x_vals, label=f\"{omega_0=}; {beta=}; {shift=}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Disp\")\n",
    "plt.axvline(x=0, linestyle='dotted', color='black')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters = dict(omega_0=omega_0, beta=beta, shift=2)\n",
    "print(injection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "t_vals = np.linspace(-1, 10, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = damped_sho(t_vals, **injection_parameters) + np.random.normal(0, sigma, t_vals.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_vals, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(t_vals, damped_sho(t_vals, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.prior import Uniform, DeltaFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['omega_0'] = Uniform(0.1, 2, name='omega_0', latex_label='$\\omega_0$') #np array\n",
    "priors['beta'] = Uniform(0, 0.5, name='beta', latex_label='$\\\\beta$') #np array\n",
    "priors['shift'] = Uniform(-4, 4, name='shift', latex_label='$\\Delta\\;t$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_data(omega_0=None, beta=None, shift=None, num_points=1):\n",
    "    \"\"\"Sample omega, beta, shift and return a batch of data with noise\"\"\"\n",
    "    omega_0 = priors['omega_0'].sample() if omega_0 is None else omega_0\n",
    "    omega_0 = torch.tensor(omega_0).to(dtype=torch.float32)\n",
    "    beta = priors['beta'].sample() if beta is None else beta\n",
    "    beta = torch.tensor(beta).to(dtype=torch.float32)\n",
    "    shift = priors['shift'].sample() if shift is None else shift\n",
    "    shift = torch.tensor(shift).to(dtype=torch.float32)\n",
    "\n",
    "    t_vals = torch.linspace(-1, 10, num_points).to(dtype=torch.float32) #\n",
    "\n",
    "    y = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    y += sigma * torch.randn(size=y.size()).to(dtype=torch.float32)\n",
    "\n",
    "    return t_vals, y, omega_0, beta, shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_simulations = 100000\n",
    "num_repeats = 10\n",
    "\n",
    "theta_unshifted_vals = []\n",
    "theta_shifted_vals = []\n",
    "data_unshifted_vals = []\n",
    "data_shifted_vals = []\n",
    "\n",
    "for ii in range(num_simulations):\n",
    "    # generated data with a fixed shift\n",
    "    t_vals, y_unshifted, omega, beta, shift = get_data(num_points=num_points, shift=1)\n",
    "    # create repeats\n",
    "    theta_unshifted = torch.tensor([omega, beta, shift]).repeat(num_repeats, 1).to(device=device)\n",
    "    theta_unshifted_vals.append(theta_unshifted)\n",
    "    data_unshifted_vals.append(y_unshifted.repeat(num_repeats, 1).to(device=device))\n",
    "    # generate shifted data\n",
    "    theta_shifted = []\n",
    "    data_shifted = []\n",
    "    for _ in range(num_repeats):\n",
    "        t_val, y_shifted, _omega, _beta, shift = get_data(\n",
    "            omega_0=omega, beta=beta,  # omega and beta same as above\n",
    "            shift=None,\n",
    "            num_points=num_points\n",
    "        )\n",
    "        theta_shifted.append(torch.tensor([omega, beta, shift]))\n",
    "        data_shifted.append(y_shifted)\n",
    "    theta_shifted_vals.append(torch.stack(theta_shifted).to(device=device))\n",
    "    data_shifted_vals.append(torch.stack(data_shifted).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (\n",
    "            theta_shifted_vals[idx].to(dtype=torch.float32),\n",
    "            theta_unshifted_vals[idx].to(dtype=torch.float32),\n",
    "            data_shifted_vals[idx].to(dtype=torch.float32),\n",
    "            data_unshifted_vals[idx].to(dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, t, d, _ = dataset[6]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (omega, beta, shift), points in zip(t, d):\n",
    "    ax.plot(t_vals.clone().detach().cpu().numpy(), points.clone().detach().cpu().numpy(),\n",
    "            'o', markersize=0.8)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f\"Augmented sample; $\\\\omega_0$ = {t[0][0]:.1f}; $\\\\beta$ = {t[0][1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_data, batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta, _, data_aug, data_orig in train_data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_loader), len(val_data_loader), len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.shape, data_aug.shape, data_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  No Embedding Net used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.distributions import StandardNormal\n",
    "from nflows.flows import Flow\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms import CompositeTransform, RandomPermutation\n",
    "\n",
    "import nflows.utils as torchutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transforms = 5\n",
    "num_blocks = 4\n",
    "hidden_features = 50\n",
    "\n",
    "context_features = num_points\n",
    "\n",
    "base_dist = StandardNormal([2])\n",
    "\n",
    "transforms = []\n",
    "for _ in range(num_transforms):\n",
    "    block = [\n",
    "        MaskedAffineAutoregressiveTransform(\n",
    "            features=2,  # 2-dim posterior\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=context_features,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=torch.tanh,\n",
    "            use_batch_norm=False,\n",
    "            use_residual_blocks=True,\n",
    "            dropout_probability=0.01,\n",
    "        ),\n",
    "        RandomPermutation(features=2)\n",
    "    ]\n",
    "    transforms += block\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of trainable parameters: \", sum(p.numel() for p in flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_augmentations = 10\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(train_data_loader, 1):\n",
    "        augmented_theta, _, augmented_data, _ = val\n",
    "        augmented_theta = augmented_theta[...,0:2]\n",
    "\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            flow_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 10 == 0:\n",
    "            last_loss = running_loss / 10 # avg loss\n",
    "            print(' Avg. train loss/batch after {} batches = {:.4f}'.format(idx, last_loss))\n",
    "            tb_x = epoch_index * len(train_data_loader) + idx\n",
    "            tb_writer.add_scalar('Flow Loss/train', last_loss, tb_x)\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def val_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(val_data_loader, 1):\n",
    "        augmented_theta, _, augmented_data, _ = val\n",
    "        augmented_theta = augmented_theta[...,0:2]\n",
    "\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 5 == 0:\n",
    "            last_loss = running_loss / 5\n",
    "            tb_x = epoch_index * len(val_data_loader) + idx + 1\n",
    "            tb_writer.add_scalar('Flow Loss/val', last_loss, tb_x)\n",
    "\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    tb_writer.flush()\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(flow.parameters(), lr=1e-3, momentum=0.5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"damped-harmonic-oscillator-baseline.neurips\", comment=\"With LR=1e-3\", flush_secs=5)\n",
    "# epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH2 = './damped-harmonic-oscillator-baseline.neurips.pth'\n",
    "flow.load_state_dict(torch.load(PATH2, map_location=device))\n",
    "flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "    # Gradient tracking\n",
    "    flow.train(True)\n",
    "    # flow._embedding_net.train(False)\n",
    "    # no gradient tracking for embedding layer\n",
    "    for name, param in flow._embedding_net.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    avg_train_loss = train_one_epoch(epoch, writer)\n",
    "\n",
    "    # no gradient tracking, for validation\n",
    "    flow.train(False)\n",
    "    avg_val_loss = val_one_epoch(epoch, writer)\n",
    "\n",
    "    print(f\"Train/Val flow Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH2 = './damped-harmonic-oscillator-baseline.notebook.pth'\n",
    "# torch.save(flow.state_dict(), PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys({'omega_0', 'beta'})\n",
    "    injections['omega_0'] = float(truth.numpy()[0])\n",
    "    injections['beta'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys({'omega_0', 'beta'})\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['omega_0'] = samples_numpy.T[0].flatten()\n",
    "    posterior['beta'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "\n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot_samples(samples, truth):\n",
    "    print(truth)\n",
    "    clear_output(wait=True)\n",
    "    sleep(0.5)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.05, 0.5, 0.95],\n",
    "        show_titles=True,\n",
    "        labels=[\"omega_0\", \"beta\",],\n",
    "        truth=truth,\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, (_, theta_test, data_test, data_orig) in enumerate(test_data_loader):\n",
    "    if idx % 1000 !=0: continue \n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0][0].reshape((1, 200)))\n",
    "    live_plot_samples(samples.cpu().reshape(3000,2), theta_test[0][0].cpu()[...,0:2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PP plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, (_, theta_test, data_test, data_unshifted) in enumerate(test_data_loader):\n",
    "    if idx == 1000: break  # full set takes time\n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0][0].reshape((1, 200)))\n",
    "    results.append(\n",
    "        cast_as_bilby_result(samples.cpu().reshape(3000, 2),\n",
    "                             theta_test[0][0].cpu()[...,0:2]))\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    bilby.result.make_pp_plot(results, save=False, keys=['omega_0', 'beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with bilby and analytic result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "bilby_result = bilby.result.Result.from_json('./bilby_sho.json', outdir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilby_result.injection_parameters.pop('shift')\n",
    "injection_parameters = bilby_result.injection_parameters.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_sim_result = bilby.result.Result.from_json(filename='./sim_flow_sho.json', outdir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = damped_sho_bilby(t_vals, **injection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x_vals = torch.from_numpy(x_vals).to(device, dtype=torch.float32)\n",
    "except TypeError:\n",
    "    x_vals = x_vals.to(device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flow_samples = flow.sample(3000, context=x_vals.reshape((1, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_result = cast_as_bilby_result(flow_samples.cpu().reshape(3000, 2),\n",
    "                                   torch.tensor(list(injection_parameters.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bilby.result.plot_multiple(\n",
    "    [bilby_result, flow_sim_result, flow_result],\n",
    "    labels=['Nested Samp.', 'Flow with Rep. (99K params)', 'Flow Baseline. (355K params)'],\n",
    "    truth=injection_parameters,\n",
    "    quantiles=(0.16, 0.84),\n",
    "    titles=True, save=True,\n",
    "    filename='sho-comparison-bilby-with-sim.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, c):\n",
    "    return m*x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "injection_parameters = dict(m=0.8, c=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "x = np.linspace(-4, 4, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = linear_model(x, **injection_parameters) + np.random.normal(0, sigma, x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(x, linear_model(x, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m_hat = (x * data).sum() / (x * x).sum()\n",
    "c_hat = data.sum() / num_points\n",
    "\n",
    "delta_m = sigma/np.sqrt(np.sum((x)**2))\n",
    "delta_c = sigma * np.sqrt(1/num_points)\n",
    "\n",
    "print(\"Expected m = {:.3f} +/- {:.3f}\".format(m_hat, delta_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Expected c = {:.3f} +/- {:.3f}\".format(c_hat, delta_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.likelihood import GaussianLikelihood\n",
    "from bilby.core.prior import Uniform, DeltaFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['m'] = Uniform(-3, 3, name='m', latex_label='m')\n",
    "priors['c'] = Uniform(-3, 3, name='c', latex_label='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "log_l = GaussianLikelihood(x, data, linear_model, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result = bilby.run_sampler(\n",
    "        likelihood=log_l, priors=priors, sampler='dynesty',\n",
    "        nlive=500, npool=4, save=False, clean=True,\n",
    "        injection_parameters=injection_parameters,\n",
    "        outdir='./linear_regression',\n",
    "        label='linear_regression'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result.plot_corner(priors=True, quantiles=(0.16, 0.84))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Same problem using likelihood-free inference\n",
    "By posterior estimation using a normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can simulate signals\n",
    "  - Given $\\mathbf{\\Theta}_i \\sim p(\\mathbf{\\Theta}) \\rightarrow$ generate signal $f(\\mathbf{\\Theta}_i)$\n",
    "  - Add instrument noise $f(\\mathbf{\\Theta}_i) + n \\rightarrow \\mathbf{d}_i$\n",
    "- We can \"easily\" get pairs $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- From $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\} \\rightarrow p(\\mathbf{\\Theta}, \\mathbf{d}), p(\\mathbf{\\Theta}\\vert \\mathbf{d}), p(\\mathbf{d}\\vert\\mathbf{\\Theta})$\n",
    "- Here, we are interested in the posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The distributions are complex\n",
    "- A solution is to use a Normalizing flow\n",
    "  - Contains a **learnable transform** (a trainable neural network)\n",
    "  - A **base distribution** (often taken to be normal)\n",
    "- Here we use a affine-autogressive transform from `pyro`\n",
    "- A standard normal base distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot_samples(samples, truth):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.16, 0.5, 0.84],\n",
    "        show_titles=True, labels=[\"m\", \"c\"],\n",
    "        truth=truth\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")\n",
    "\n",
    "\n",
    "def live_plot_bilby_result(result, **kwargs):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    result.plot_corner(priors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(m=None, c=None, num_points=1):\n",
    "    \"\"\"Sample m, c and return a batch of data with noise\"\"\"\n",
    "    m = priors['m'].sample() if m is None else m\n",
    "    c = priors['c'].sample() if c is None else c\n",
    "    x = np.linspace(-4, 4, num_points)\n",
    "    y = m*x + c\n",
    "    y += sigma*np.random.normal(size=x.size)\n",
    "\n",
    "    return x, y, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Generate simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lightning import pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 40000\n",
    "theta_vals = []\n",
    "data_vals = []\n",
    "for ii in range(num_simulations):\n",
    "    x_val, y_val, m_val, c_val = get_data(num_points=num_points)\n",
    "    data_vals.append(y_val)\n",
    "    theta_vals.append([m_val, c_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "theta_vals = torch.from_numpy(np.array(theta_vals)).to(torch.float32)\n",
    "data_vals = torch.from_numpy(np.array(data_vals)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return theta_vals[idx], data_vals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 1\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyro.nn import ConditionalAutoRegressiveNN\n",
    "from pyro.distributions import ConditionalTransformedDistribution\n",
    "from pyro.distributions.transforms import ConditionalAffineAutoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MADE in Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick recap of terminology\n",
    "- **Dataset**: the entire dataset, tuples of $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- **Dataloader**: partition of dataset into batches i.e. gives a batch of data\n",
    "- **Training loop**: One epoch\n",
    "  - Push a batch of data\n",
    "  - Calculate loss\n",
    "  - Compute all gradients\n",
    "  - Change all weights based on gradients\n",
    "  - Repeats until all batches are done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pytorch-lightning removes boilerplate code\n",
    "- `torch.nn.Module` $\\rightarrow$ `pl.LightningModule`\n",
    "- Provides methods: `training_step`, `validation_step`, `test_step`, `configure_optimizers`\n",
    "- No need of explicit training loop\n",
    "- Automatic checkpoints, several options for logging\n",
    "- Easily scales to multi-GPU distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys(injection_parameters)\n",
    "    injections['m'] = float(truth.numpy()[0])\n",
    "    injections['c'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys(injection_parameters)\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['m'] = samples_numpy.T[0].flatten()\n",
    "    posterior['c'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "    \n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MADE(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        context_dim,\n",
    "        hidden_dim,\n",
    "        learning_rate: float = LR,\n",
    "        batch_size: int = TRAIN_BATCH_SIZE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n",
    "        self.hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)\n",
    "        self.transform = ConditionalAffineAutoregressive(self.hypernet)\n",
    "        self.flow = ConditionalTransformedDistribution(self.base_dist, [self.transform])\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def log_prob(self, theta, data):\n",
    "        return self.flow.condition(data).log_prob(theta).mean()\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        samples = self.flow.condition(data).sample([2000])\n",
    "        res = cast_as_bilby_result(samples, theta[0])\n",
    "        self.test_results.append(res)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = self.transform.parameters()\n",
    "        optimizer = torch.optim.AdamW(parameters, self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.learning_rate,\n",
    "            pct_start=0.1,\n",
    "            total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        scheduler_config = dict(scheduler=scheduler, interval=\"step\")\n",
    "        return dict(optimizer=optimizer, lr_scheduler=scheduler_config)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PPPlotCallback(pl.Callback):\n",
    "    def on_test_start(self, trainer, pl_module):\n",
    "        pl_module.test_results = []\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            bilby.result.make_pp_plot(pl_module.test_results, save=False, keys=['m', 'c'])\n",
    "        pl_module.test_results.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "context_dim = num_points\n",
    "hidden_dims = [5*input_dim, 5*input_dim]\n",
    "\n",
    "model = MADE(input_dim, context_dim, hidden_dims, batch_size=40, learning_rate=5e-3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    log_every_n_steps=100,\n",
    "    logger=pl.loggers.CSVLogger(\"logs\", name=\"made-expt\"),\n",
    "    callbacks=[PPPlotCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Example posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (theta_test, data_test) in enumerate(test_data):\n",
    "    if idx == 5: break \n",
    "    with torch.no_grad():\n",
    "        samples = model.flow.condition(data_test).sample([1000])\n",
    "    live_plot_samples(samples, theta_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flow_samples = model.flow.condition(\n",
    "        torch.from_numpy(data).unsqueeze(0).to(dtype=torch.float32)\n",
    "    ).sample([1000])\n",
    "truth = np.array(list(injection_parameters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "live_plot_samples(flow_samples, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
